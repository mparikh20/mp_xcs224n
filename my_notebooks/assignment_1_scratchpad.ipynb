{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a511f6d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb45ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d5f2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c46b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b394ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_dir = cwd.parent / 'XCS224N-A1-master' / 'src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d28d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(stanford_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7ff15a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/mukti/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebce40d",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9fdbe",
   "metadata": {},
   "source": [
    "## Problem 1, function 1 - get unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebde5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "()            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = []\n",
    "    num_corpus_words = 0\n",
    "\n",
    "    # ### START CODE HERE ###\n",
    "    corpus_words = sorted(list({item for document in corpus for item in document if item.isalpha() or item in ['<START>','<END>']}))\n",
    "    num_corpus_words = len(corpus_words)\n",
    "    # ### END CODE HERE ###\n",
    "\n",
    "    return corpus_words, num_corpus_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab6016",
   "metadata": {},
   "source": [
    "**Create an example corpus containing just 2 lists in it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3915bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "c1 = '<START> all that glitters is not gold . <END>'\n",
    "c2 = '<START> all is well that ends well . <END>'\n",
    "\n",
    "corpus = [c1.split(), c2.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d23e850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<START>', 'all', 'that', 'glitters', 'is', 'not', 'gold', '.', '<END>'],\n",
       " ['<START>', 'all', 'is', 'well', 'that', 'ends', 'well', '.', '<END>']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c499e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words, num_corpus_words = distinct_words(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b220cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<END>',\n",
       " '<START>',\n",
       " 'all',\n",
       " 'ends',\n",
       " 'glitters',\n",
       " 'gold',\n",
       " 'is',\n",
       " 'not',\n",
       " 'that',\n",
       " 'well']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f9be71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "216817fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_corpus = read_corpus()[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a49f801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START>',\n",
       " 'texaco',\n",
       " '&',\n",
       " 'lt',\n",
       " ';',\n",
       " 'txc',\n",
       " '>',\n",
       " 'canada',\n",
       " 'to',\n",
       " 'raise',\n",
       " 'crude',\n",
       " 'oil',\n",
       " 'postings',\n",
       " 'texaco',\n",
       " 'inc',\n",
       " \"'\",\n",
       " 's',\n",
       " 'texaco',\n",
       " 'canada',\n",
       " 'said',\n",
       " 'it',\n",
       " 'will',\n",
       " 'raise',\n",
       " 'postings',\n",
       " 'for',\n",
       " 'its',\n",
       " 'edmonton',\n",
       " '/',\n",
       " 'swann',\n",
       " 'hills',\n",
       " 'crude',\n",
       " 'by',\n",
       " '24',\n",
       " 'canadian',\n",
       " 'cts',\n",
       " 'a',\n",
       " 'barrel',\n",
       " ',',\n",
       " 'effective',\n",
       " 'june',\n",
       " '20',\n",
       " '.',\n",
       " 'the',\n",
       " 'company',\n",
       " 'said',\n",
       " 'the',\n",
       " 'new',\n",
       " 'posting',\n",
       " 'for',\n",
       " 'edmonton',\n",
       " '/',\n",
       " 'swann',\n",
       " 'hills',\n",
       " 'will',\n",
       " 'be',\n",
       " '25',\n",
       " '.',\n",
       " '60',\n",
       " 'dlrs',\n",
       " 'a',\n",
       " 'barrel',\n",
       " '.',\n",
       " 'the',\n",
       " 'price',\n",
       " 'hike',\n",
       " 'follows',\n",
       " 'a',\n",
       " 'round',\n",
       " 'of',\n",
       " 'crude',\n",
       " 'oil',\n",
       " 'price',\n",
       " 'increases',\n",
       " 'started',\n",
       " 'late',\n",
       " 'june',\n",
       " '17',\n",
       " 'by',\n",
       " 'sun',\n",
       " 'co',\n",
       " '.',\n",
       " 'the',\n",
       " 'other',\n",
       " 'major',\n",
       " 'canadian',\n",
       " 'crude',\n",
       " 'suppliers',\n",
       " 'raised',\n",
       " 'prices',\n",
       " 'june',\n",
       " '18',\n",
       " '.',\n",
       " '<END>']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfefd0",
   "metadata": {},
   "source": [
    "## Problem 1, function 2 - get co-occurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "640c144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
    "\n",
    "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
    "              number of co-occurring words.\n",
    "\n",
    "              For example, if we take the document \"START All that glitters is not gold END\" with window size of 4,\n",
    "              \"All\" will co-occur with \"START\", \"that\", \"glitters\", \"is\", and \"not\".\n",
    "\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)):\n",
    "                Co-occurrence matrix of word counts.\n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)\n",
    "    M = None\n",
    "    word2Ind = {}\n",
    "\n",
    "    # ### START CODE HERE ###\n",
    "    # initialize a matrix of size num_words\n",
    "    M = np.zeros((num_words,num_words))\n",
    "    \n",
    "    # create a dictionary that takes the sorted words list and then assigns an index to each\n",
    "    # This dict will help in mapping the word pairs to add values in the M matrix\n",
    "    word2Ind = dict((word,n) for n, word in enumerate(words))\n",
    "    \n",
    "    # create a clean corpus structure that eliminates characters but keeps the list of lists\n",
    "    cleaned_corpus = [[item for item in document if item.isalpha() or item in ['<START>','<END>']] for document in corpus]\n",
    "    \n",
    "    # cleaned_corpus is a list of lists. Each list is a document.\n",
    "    # Each list has strings\n",
    "    for document in cleaned_corpus:\n",
    "        # loop over each word that would be a center word\n",
    "        for center_i in range(len(document)):\n",
    "            # this will only select indices that span the window around the center word and within range\n",
    "            # context_m is the index of a word present around the center word in a window of size window_size\n",
    "            # get the center word and context word\n",
    "            center_word = document[center_i]\n",
    "            \n",
    "            for context_m in filter(lambda x: 0 <= x <= len(document)-1 and x != center_i, \n",
    "                                map(lambda x: x + center_i, range(-window_size,window_size+1))):\n",
    "                \n",
    "                # get context word\n",
    "                context_word = document[context_m]         \n",
    "                \n",
    "                # access these words from the dictionary\n",
    "                # Allot the row index of M to be for center words and column index for the context word\n",
    "                if center_word in word2Ind and context_word in word2Ind:\n",
    "                    M[word2Ind[center_word],word2Ind[context_word]] += 1\n",
    "                    \n",
    "    # ### END CODE HERE ###\n",
    "\n",
    "    return M, word2Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74557f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<START>', 'all', 'that', 'glitters', 'is', 'not', 'gold', '.', '<END>'],\n",
       " ['<START>', 'all', 'is', 'well', 'that', 'ends', 'well', '.', '<END>']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "098a5109",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M, word2Ind = compute_co_occurrence_matrix(corpus,window_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a37716b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 1., 1., 1., 1., 1., 2.],\n",
       "       [0., 0., 2., 0., 1., 0., 2., 0., 2., 1.],\n",
       "       [0., 2., 0., 1., 1., 0., 2., 1., 2., 1.],\n",
       "       [1., 0., 1., 0., 0., 0., 1., 0., 1., 2.],\n",
       "       [1., 1., 1., 0., 0., 1., 1., 1., 1., 0.],\n",
       "       [1., 0., 0., 0., 1., 0., 1., 1., 1., 0.],\n",
       "       [1., 2., 2., 1., 1., 1., 0., 1., 2., 2.],\n",
       "       [1., 0., 1., 0., 1., 1., 1., 0., 1., 0.],\n",
       "       [1., 2., 2., 1., 1., 1., 2., 1., 0., 2.],\n",
       "       [2., 1., 1., 2., 0., 0., 2., 0., 2., 2.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a882c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<END>': 0,\n",
       " '<START>': 1,\n",
       " 'all': 2,\n",
       " 'ends': 3,\n",
       " 'glitters': 4,\n",
       " 'gold': 5,\n",
       " 'is': 6,\n",
       " 'not': 7,\n",
       " 'that': 8,\n",
       " 'well': 9}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2Ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d70f4",
   "metadata": {},
   "source": [
    "## Problem 1, function 3 - transform matrix using SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecff93cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6fcb9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce a co-occurrence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "\n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurrence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of unique words in the corpus, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"\n",
    "    np.random.seed(4355)\n",
    "    n_iter = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "\n",
    "    # ### START CODE HERE ###\n",
    "    svd = TruncatedSVD(n_components=k,n_iter=n_iter)\n",
    "    \n",
    "    # Transform the matrix\n",
    "    M_reduced = svd.fit_transform(M)\n",
    "    # ### END CODE HERE ###\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return M_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be5f24a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Truncated SVD over 10 words...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "M_reduced = reduce_to_k_dim(M,k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d582cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30ccb692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.51757677, -0.09706068],\n",
       "       [ 3.04504687,  1.64023541],\n",
       "       [ 3.39994784,  0.16221166],\n",
       "       [ 2.32577504,  0.52854231],\n",
       "       [ 2.11328485, -0.18879363],\n",
       "       [ 1.51131468,  0.79361344],\n",
       "       [ 4.10492166, -1.33451162],\n",
       "       [ 1.82887385,  0.70520148],\n",
       "       [ 4.10492166, -1.33451162],\n",
       "       [ 4.22388595,  0.5528    ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05ef734d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1a6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e07b39a",
   "metadata": {},
   "source": [
    "## Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9e94880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2, -1, 0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x + 2 ,range(-4,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d56adac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n",
      "-1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for n in map(lambda x: x + 2 ,range(-4,5)):\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4de0694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5a844d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "20b17530",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {'cest':0,\n",
    "             'un': 1,\n",
    "             'dimanche':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "48aa68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[test_dict['cest'],test_dict['un']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9fd49cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a2a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
